package cmd

import (
	"bufio"
	"context"
	"fmt"
	"log"
	"net/url"
	"os"
	"time"

	"github.com/shouni/go-web-exact/v2/pkg/extract"
	"github.com/spf13/cobra"
)

var rawUrl string

// ğŸ’¡ ä¿®æ­£ç‚¹1: ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã‚’å®šæ•°åŒ–
const defaultOverallTimeoutIfClientTimeoutIsZero = 20 * time.Second

// runExtractionPipeline ã¯ã€Webã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡ºã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã§ã™ã€‚
func runExtractionPipeline(rawURL string, extractor *extract.Extractor, overallTimeout time.Duration) (text string, isBodyExtracted bool, err error) {
	// 1. å…¨ä½“å‡¦ç†ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š
	ctx, cancel := context.WithTimeout(context.Background(), overallTimeout)
	defer cancel()

	// 2. æŠ½å‡ºã®å®Ÿè¡Œ
	text, isBodyExtracted, err = extractor.FetchAndExtractText(ctx, rawURL)
	if err != nil {
		// ã‚¨ãƒ©ãƒ¼ã®ãƒ©ãƒƒãƒ”ãƒ³ã‚°
		return "", false, fmt.Errorf("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼ (URL: %s): %w", rawURL, err)
	}

	return text, isBodyExtracted, nil
}

// ensureScheme ã¯ã€URLã®ã‚¹ã‚­ãƒ¼ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã« https:// ã‚’è£œå®Œã—ã¾ã™ã€‚
// æ—¢ã«ã‚¹ã‚­ãƒ¼ãƒ ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãã‚ŒãŒ http ã¾ãŸã¯ https ã§ã‚ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚
func ensureScheme(rawURL string) (string, error) {
	// 1. ã¾ãšç¾åœ¨ã®URLã‚’ãƒ‘ãƒ¼ã‚¹
	parsedURL, err := url.Parse(rawURL)
	if err != nil {
		return "", fmt.Errorf("URLã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: %w", err)
	}

	// 2. ã‚¹ã‚­ãƒ¼ãƒ ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã®ãƒã‚§ãƒƒã‚¯
	if parsedURL.Scheme != "" {
		if parsedURL.Scheme != "http" && parsedURL.Scheme != "https" {
			return "", fmt.Errorf("ç„¡åŠ¹ãªURLã‚¹ã‚­ãƒ¼ãƒ ã§ã™ã€‚httpã¾ãŸã¯httpsã‚’æŒ‡å®šã—ã¦ãã ã•ã„: %s", rawURL)
		}
		// æ—¢å­˜ã®ã‚¹ã‚­ãƒ¼ãƒ ã‚’å°Šé‡
		return rawURL, nil
	}

	// 3. ã‚¹ã‚­ãƒ¼ãƒ ãŒãªã„å ´åˆã€HTTPSã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã¦ä»˜ä¸
	// ã‚¹ã‚­ãƒ¼ãƒ ãªã—ã§å…¥åŠ›ã•ã‚ŒãŸå ´åˆã€HTTPSã‚’å„ªå…ˆã—ã¾ã™ã€‚HTTPã‚’æ„å›³ã™ã‚‹å ´åˆã¯æ˜ç¤ºçš„ã« http:// ã‚’ä»˜ä¸ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
	return "https://" + rawURL, nil
}

var extractCmd = &cobra.Command{
	Use:   "extract [URL]",
	Short: "æŒ‡å®šã•ã‚ŒãŸURLã¾ãŸã¯æ¨™æº–å…¥åŠ›ã‹ã‚‰Webã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã™",
	Long:  `æŒ‡å®šã•ã‚ŒãŸURLã¾ãŸã¯æ¨™æº–å…¥åŠ›ã‹ã‚‰Webã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚`,
	RunE: func(cmd *cobra.Command, args []string) error {

		// overallTimeout ã®è¨­å®š: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (Flags.TimeoutSec) ã®2å€ã‚’å…¨ä½“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ã—ã¾ã™ã€‚
		// ğŸ’¡ ä¿®æ­£ç‚¹2: ä¸è¦ãªã‚³ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã€è¨ˆç®—å¼ã‚’ç°¡ç´ åŒ– (è¡Œ 68)
		overallTimeout := time.Duration(Flags.TimeoutSec*2) * time.Second
		if Flags.TimeoutSec == 0 {
			// ğŸ’¡ ä¿®æ­£ç‚¹3: ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã‚’å®šæ•°ã«ç½®ãæ›ãˆ (è¡Œ 70)
			overallTimeout = defaultOverallTimeoutIfClientTimeoutIsZero
		}

		// 1. å‡¦ç†å¯¾è±¡URLã®æ±ºå®š (ãƒ•ãƒ©ã‚°å„ªå…ˆ)
		urlToProcess := rawUrl
		if urlToProcess == "" {
			log.Println("URLãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€æ¨™æº–å…¥åŠ›ã‹ã‚‰URLã‚’èª­ã¿è¾¼ã¿ã¾ã™...")
			scanner := bufio.NewScanner(os.Stdin)
			fmt.Print("å‡¦ç†ã™ã‚‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")

			if !scanner.Scan() {
				if err := scanner.Err(); err != nil {
					return fmt.Errorf("æ¨™æº–å…¥åŠ›ã®èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: %w", err)
				}
				return fmt.Errorf("URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
			}
			urlToProcess = scanner.Text()
		}

		// 2. URLã®ã‚¹ã‚­ãƒ¼ãƒ è£œå®Œã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
		processedURL, err := ensureScheme(urlToProcess)
		if err != nil {
			return fmt.Errorf("URLã‚¹ã‚­ãƒ¼ãƒ ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: %w", err)
		}
		log.Printf("å‡¦ç†å¯¾è±¡URL: %s (å…¨ä½“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: %s)\n", processedURL, overallTimeout)

		// 3. ä¾å­˜æ€§ã®åˆæœŸåŒ–
		// cmd/root.go ã§åˆæœŸåŒ–ã•ã‚ŒãŸå…±æœ‰ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã‚’ä½¿ç”¨ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã® --timeout ã¨ --max-retries ãŒåæ˜ ã•ã‚Œã¾ã™ã€‚
		fetcher := GetGlobalFetcher()
		if fetcher == nil {
			return fmt.Errorf("HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚rootã‚³ãƒãƒ³ãƒ‰ã®PreRunã‚’ç¢ºèªã—ã¦ãã ã•ã„")
		}

		// ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¨˜æ†¶ã«ã‚ã‚‹ extract ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã® NewExtractor ã‚’åˆ©ç”¨
		extractor, err := extract.NewExtractor(fetcher)
		if err != nil {
			return fmt.Errorf("Extractorã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: %w", err)
		}

		// 4. ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè¡Œ
		text, isBodyExtracted, err := runExtractionPipeline(processedURL, extractor, overallTimeout)
		if err != nil {
			// ğŸ’¡ ä¿®æ­£ç‚¹4: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã« processedURL æƒ…å ±ã‚’å«ã‚ã‚‹ (è¡Œ 108)
			return fmt.Errorf("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ (URL: %s): %w", processedURL, err)
		}

		// 5. çµæœã®å‡ºåŠ›
		if !isBodyExtracted {
			fmt.Printf("æœ¬æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã—ã¾ã—ãŸ:\n%s\n", text)
		} else {
			fmt.Println("--- æŠ½å‡ºã•ã‚ŒãŸæœ¬æ–‡ ---")
			fmt.Println(text)
			fmt.Println("-----------------------")
		}

		return nil
	},
}

func init() {
	extractCmd.Flags().StringVarP(&rawUrl, "url", "u", "", "æŠ½å‡ºå¯¾è±¡ã®URL")
}
