package cmd

import (
	"bufio"
	"context"
	"fmt"
	"log"
	"os"
	"strings"
	"time"

	"github.com/shouni/go-web-exact/v2/pkg/extract"
	"github.com/shouni/go-web-exact/v2/pkg/scraper"
	"github.com/spf13/cobra"
)

// ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ•ãƒ©ã‚°å¤‰æ•°ã‚’å®šç¾©
var (
	inputURLs   string // --urls ãƒ•ãƒ©ã‚°ã§å—ã‘å–ã‚‹ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®URLãƒªã‚¹ãƒˆ
	concurrency int    // --concurrency ãƒ•ãƒ©ã‚°ã§å—ã‘å–ã‚‹ä¸¦åˆ—å®Ÿè¡Œæ•°
)

// runScrapePipeline ã¯ã€ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã§ã™ã€‚
func runScrapePipeline(urls []string, extractor *extract.Extractor, concurrency int) {

	// 1. Scraperã®åˆæœŸåŒ– (NewParallelScraper ã‚’åˆ©ç”¨)
	scraper := scraper.NewParallelScraper(extractor, concurrency)

	// 2. ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š: (ä¿®æ­£ç‚¹1ã«å¯¾å¿œ)
	// ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ(Flags.TimeoutSec)ã‚’åŸºã«å…¨ä½“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨ˆç®—ã—ã€ä¸€è²«æ€§ã‚’ä¿ã¤ã€‚
	var clientTimeout time.Duration
	if Flags.TimeoutSec == 0 {
		clientTimeout = defaultTimeoutSec * time.Second
	} else {
		clientTimeout = time.Duration(Flags.TimeoutSec) * time.Second
	}
	// extractorCmdã¨åŒæ§˜ã«ã€å…¨ä½“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®2å€ã¨ã™ã‚‹
	overallTimeout := clientTimeout * 2

	// 3. å…¨ä½“å‡¦ç†ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š
	ctx, cancel := context.WithTimeout(context.Background(), overallTimeout)
	defer cancel()

	log.Printf("ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ (å¯¾è±¡URLæ•°: %d, æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°: %d, å…¨ä½“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: %s)\n",
		len(urls), concurrency, overallTimeout)

	// 4. ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè¡Œ
	results := scraper.ScrapeInParallel(ctx, urls)

	// 5. çµæœã®å‡ºåŠ›
	fmt.Println("--- ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ ---")

	successCount := 0
	errorCount := 0

	for i, res := range results {
		if res.Error != nil {
			errorCount++
			fmt.Printf("âŒ [%d] %s\n", i+1, res.URL)
			fmt.Printf("     ã‚¨ãƒ©ãƒ¼: %v\n", res.Error)
		} else {
			successCount++
			fmt.Printf("âœ… [%d] %s\n", i+1, res.URL)
			fmt.Printf("     æŠ½å‡ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é•·ã•: %d æ–‡å­—\n", len(res.Content))

			// ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤º
			if len(res.Content) > 100 {
				fmt.Printf("     ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: %s...\n", res.Content[:100])
			} else {
				fmt.Printf("     ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: %s\n", res.Content)
			}
		}
	}

	fmt.Println("-------------------------------")
	fmt.Printf("å®Œäº†: æˆåŠŸ %d ä»¶, å¤±æ•— %d ä»¶\n", successCount, errorCount)
}

// scrapeCmd ã‹ã‚‰ scraperCmd ã«åç§°å¤‰æ›´
var scraperCmd = &cobra.Command{
	Use:   "scraper",
	Short: "è¤‡æ•°ã®URLã‚’ä¸¦åˆ—ã§å‡¦ç†ã—ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã—ã¾ã™",
	Long:  `--urls ãƒ•ãƒ©ã‚°ã§ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®URLãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ã‹ã€æ¨™æº–å…¥åŠ›ã‹ã‚‰URLã‚’ä¸€è¡Œãšã¤èª­ã¿è¾¼ã¿ã€æŒ‡å®šã•ã‚ŒãŸæœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°ã§ä¸¦åˆ—æŠ½å‡ºã‚’å®Ÿè¡Œã—ã¾ã™ã€‚`,
	Args:  cobra.NoArgs, // ä½ç½®å¼•æ•°ã¯å–ã‚‰ãªã„

	RunE: func(cmd *cobra.Command, args []string) error {

		// 1. ä¾å­˜æ€§ã®åˆæœŸåŒ– (Fetcher -> Extractor)
		fetcher := GetGlobalFetcher()
		if fetcher == nil {
			return fmt.Errorf("HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
		}
		extractor, err := extract.NewExtractor(fetcher)
		if err != nil {
			return fmt.Errorf("Extractorã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: %w", err)
		}

		// 2. å‡¦ç†å¯¾è±¡URLã®ãƒªã‚¹ãƒˆã‚’æ±ºå®š (ä¿®æ­£ç‚¹2ã«å¯¾å¿œ: ensureSchemeã‚’é©ç”¨)
		var urls []string
		var rawURLs []string

		// 2-1. ãƒ•ãƒ©ã‚°ã‹ã‚‰ã®èª­ã¿è¾¼ã¿
		if inputURLs != "" {
			rawURLs = strings.Split(inputURLs, ",")
		} else {
			// 2-2. æ¨™æº–å…¥åŠ›ã‹ã‚‰ã®èª­ã¿è¾¼ã¿
			log.Println("URLãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€æ¨™æº–å…¥åŠ›ã‹ã‚‰URLã‚’èª­ã¿è¾¼ã¿ã¾ã™ (Ctrl+Dã¾ãŸã¯EOFã§çµ‚äº†)...")
			scanner := bufio.NewScanner(os.Stdin)
			for scanner.Scan() {
				rawURLs = append(rawURLs, scanner.Text())
			}
			if err := scanner.Err(); err != nil {
				return fmt.Errorf("æ¨™æº–å…¥åŠ›ã®èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: %w", err)
			}
		}

		// 2-3. URLã‚¹ã‚­ãƒ¼ãƒ è£œå®Œã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®é©ç”¨
		for _, u := range rawURLs {
			u = strings.TrimSpace(u)
			if u != "" {
				// ğŸ’¡ ensureScheme ã‚’å‘¼ã³å‡ºã™
				processed, err := ensureScheme(u)
				if err != nil {
					return fmt.Errorf("URLã‚¹ã‚­ãƒ¼ãƒ ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼ (%s): %w", u, err)
				}
				urls = append(urls, processed)
			}
		}

		if len(urls) == 0 {
			return fmt.Errorf("å‡¦ç†å¯¾è±¡ã®URLãŒä¸€ã¤ã‚‚æŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
		}

		// 3. ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè¡Œ
		runScrapePipeline(urls, extractor, concurrency)

		return nil
	},
}

func init() {
	// --urls ãƒ•ãƒ©ã‚°: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®URLãƒªã‚¹ãƒˆ
	scraperCmd.Flags().StringVarP(&inputURLs, "urls", "u", "",
		"æŠ½å‡ºå¯¾è±¡ã®ã‚«ãƒ³ãƒåŒºåˆ‡ã‚ŠURLãƒªã‚¹ãƒˆ (ä¾‹: url1,url2,url3)")

	// --concurrency ãƒ•ãƒ©ã‚°: ä¸¦åˆ—å®Ÿè¡Œæ•°ã®æŒ‡å®š
	scraperCmd.Flags().IntVarP(&concurrency, "concurrency", "c",
		scraper.DefaultMaxConcurrency,
		fmt.Sprintf("æœ€å¤§ä¸¦åˆ—å®Ÿè¡Œæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: %d)", scraper.DefaultMaxConcurrency))
}
