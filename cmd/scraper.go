package cmd

import (
	"bufio"
	"context"
	"fmt"
	"log"
	"os"
	"strings"
	"time"

	"github.com/shouni/go-web-exact/v2/pkg/extract"
	"github.com/shouni/go-web-exact/v2/pkg/scraper"
	"github.com/spf13/cobra"
)

// ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ•ãƒ©ã‚°å¤‰æ•°ã‚’å®šç¾©
var (
	inputURLs   string // --urls ãƒ•ãƒ©ã‚°ã§å—ã‘å–ã‚‹ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®URLãƒªã‚¹ãƒˆ
	concurrency int    // --concurrency ãƒ•ãƒ©ã‚°ã§å—ã‘å–ã‚‹ä¸¦åˆ—å®Ÿè¡Œæ•°
)

// runScrapePipeline ã¯ã€ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã§ã™ã€‚
func runScrapePipeline(urls []string, extractor *extract.Extractor, concurrency int) {

	// 1. Scraperã®åˆæœŸåŒ– (NewParallelScraper ã‚’åˆ©ç”¨)
	scraper := scraper.NewParallelScraper(extractor, concurrency)

	// 2. ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®2å€ã‚’å…¨ä½“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ã—ã¾ã™ã€‚
	overallTimeout := time.Duration(Flags.TimeoutSec) * 2 * time.Second
	if Flags.TimeoutSec == 0 {
		// NOTE: ã“ã“ã§ã¯ã€ãƒ«ãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤(30ç§’)ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹ã¨ä»®å®šã—ã€æš«å®šçš„ã«30ç§’ã®2å€(60ç§’)ã¨ã™ã‚‹ã€‚
		overallTimeout = time.Duration(30) * 2 * time.Second
	}

	// 3. å…¨ä½“å‡¦ç†ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š
	ctx, cancel := context.WithTimeout(context.Background(), overallTimeout)
	defer cancel()

	log.Printf("ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ (å¯¾è±¡URLæ•°: %d, æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°: %d, å…¨ä½“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: %s)\n",
		len(urls), concurrency, overallTimeout)

	// 4. ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè¡Œ
	results := scraper.ScrapeInParallel(ctx, urls)

	// 5. çµæœã®å‡ºåŠ›
	fmt.Println("--- ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ ---")

	successCount := 0
	errorCount := 0

	for i, res := range results {
		if res.Error != nil {
			errorCount++
			fmt.Printf("âŒ [%d] %s\n", i+1, res.URL)
			fmt.Printf("     ã‚¨ãƒ©ãƒ¼: %v\n", res.Error)
		} else {
			successCount++
			fmt.Printf("âœ… [%d] %s\n", i+1, res.URL)
			fmt.Printf("     æŠ½å‡ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é•·ã•: %d æ–‡å­—\n", len(res.Content))

			// ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤º
			if len(res.Content) > 100 {
				fmt.Printf("     ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: %s...\n", res.Content[:100])
			} else {
				fmt.Printf("     ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: %s\n", res.Content)
			}
		}
	}

	fmt.Println("-------------------------------")
	fmt.Printf("å®Œäº†: æˆåŠŸ %d ä»¶, å¤±æ•— %d ä»¶\n", successCount, errorCount)
}

// ğŸ’¡ scrapeCmd ã‹ã‚‰ scraperCmd ã«åç§°å¤‰æ›´ã—ã€Useãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ "scraper" ã«å¤‰æ›´
var scraperCmd = &cobra.Command{
	Use:   "scraper",
	Short: "è¤‡æ•°ã®URLã‚’ä¸¦åˆ—ã§å‡¦ç†ã—ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã—ã¾ã™",
	Long:  `--urls ãƒ•ãƒ©ã‚°ã§ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®URLãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ã‹ã€æ¨™æº–å…¥åŠ›ã‹ã‚‰URLã‚’ä¸€è¡Œãšã¤èª­ã¿è¾¼ã¿ã€æŒ‡å®šã•ã‚ŒãŸæœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°ã§ä¸¦åˆ—æŠ½å‡ºã‚’å®Ÿè¡Œã—ã¾ã™ã€‚`,
	Args:  cobra.NoArgs, // ä½ç½®å¼•æ•°ã¯å–ã‚‰ãªã„

	RunE: func(cmd *cobra.Command, args []string) error {

		// 1. ä¾å­˜æ€§ã®åˆæœŸåŒ– (Fetcher -> Extractor)
		fetcher := GetGlobalFetcher()
		if fetcher == nil {
			return fmt.Errorf("HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
		}
		extractor, err := extract.NewExtractor(fetcher)
		if err != nil {
			return fmt.Errorf("Extractorã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: %w", err)
		}

		// 2. å‡¦ç†å¯¾è±¡URLã®ãƒªã‚¹ãƒˆã‚’æ±ºå®š
		var urls []string

		if inputURLs != "" {
			// --urls ãƒ•ãƒ©ã‚°ã‹ã‚‰URLãƒªã‚¹ãƒˆã‚’å–å¾—
			urls = strings.Split(inputURLs, ",")
		} else {
			// æ¨™æº–å…¥åŠ›ã‹ã‚‰URLã‚’ä¸€è¡Œãšã¤èª­ã¿è¾¼ã‚€
			log.Println("URLãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€æ¨™æº–å…¥åŠ›ã‹ã‚‰URLã‚’èª­ã¿è¾¼ã¿ã¾ã™ (Ctrl+Dã¾ãŸã¯EOFã§çµ‚äº†)...")
			scanner := bufio.NewScanner(os.Stdin)
			for scanner.Scan() {
				url := strings.TrimSpace(scanner.Text())
				if url != "" {
					urls = append(urls, url)
				}
			}
			if err := scanner.Err(); err != nil {
				return fmt.Errorf("æ¨™æº–å…¥åŠ›ã®èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: %w", err)
			}
		}

		if len(urls) == 0 {
			return fmt.Errorf("å‡¦ç†å¯¾è±¡ã®URLãŒä¸€ã¤ã‚‚æŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
		}

		// 3. ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè¡Œ
		runScrapePipeline(urls, extractor, concurrency)

		return nil
	},
}

func init() {
	// --urls ãƒ•ãƒ©ã‚°: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®URLãƒªã‚¹ãƒˆ
	scraperCmd.Flags().StringVarP(&inputURLs, "urls", "u", "",
		"æŠ½å‡ºå¯¾è±¡ã®ã‚«ãƒ³ãƒåŒºåˆ‡ã‚ŠURLãƒªã‚¹ãƒˆ (ä¾‹: url1,url2,url3)")

	// --concurrency ãƒ•ãƒ©ã‚°: ä¸¦åˆ—å®Ÿè¡Œæ•°ã®æŒ‡å®š
	scraperCmd.Flags().IntVarP(&concurrency, "concurrency", "c",
		scraper.DefaultMaxConcurrency,
		fmt.Sprintf("æœ€å¤§ä¸¦åˆ—å®Ÿè¡Œæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: %d)", scraper.DefaultMaxConcurrency))

	// NOTE: ã“ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ rootCmd ã«è¿½åŠ ã™ã‚‹ã«ã¯ã€root.goã§ AddCommand(scraperCmd) ã‚’å‘¼ã³å‡ºã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
}
