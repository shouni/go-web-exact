package cmd

import (
	"context"
	"fmt"
	"log"
	"time"

	"github.com/mmcdole/gofeed"
	"github.com/shouni/go-http-kit/pkg/httpkit"
	"github.com/spf13/cobra"

	"github.com/shouni/go-web-exact/v2/pkg/feed"
)

// ãƒ•ã‚£ãƒ¼ãƒ‰URLã‚’ä¿æŒã™ã‚‹ãƒ•ãƒ©ã‚°å¤‰æ•°
var feedURL string

// ãƒ•ã‚£ãƒ¼ãƒ‰ã®å…¨ä½“å‡¦ç†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š (extractCmdã¨çµ±ä¸€)
// Flags.TimeoutSec ã¯HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°ã‚’è¡¨ã—ã¾ã™ã€‚
const overallFeedTimeoutFactor = 2 // ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®2å€

// runParsePipeline ã¯ã€ãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã¨ãƒ‘ãƒ¼ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã§ã™ã€‚
func runParsePipeline(url string, parser *feed.Parser, overallTimeout time.Duration) (*gofeed.Feed, error) {
	// 1. å…¨ä½“å‡¦ç†ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š
	ctx, cancel := context.WithTimeout(context.Background(), overallTimeout)
	defer cancel()

	// 2. æŠ½å‡ºã®å®Ÿè¡Œ
	parsedFeed, err := parser.FetchAndParse(ctx, url)
	if err != nil {
		// ã‚¨ãƒ©ãƒ¼ã®ãƒ©ãƒƒãƒ”ãƒ³ã‚°
		return nil, fmt.Errorf("ãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ãŠã‚ˆã³ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ (URL: %s): %w", url, err)
	}

	return parsedFeed, nil
}

var parseCmd = &cobra.Command{
	Use:   "parse",
	Short: "RSS/Atomãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ãƒ»è§£æã—ã€ã‚¿ã‚¤ãƒˆãƒ«ã¨è¨˜äº‹ã‚’ä¸€è¦§è¡¨ç¤ºã—ã¾ã™",
	Long:  `æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰RSSã¾ãŸã¯Atomãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã€ãã®å†…å®¹ï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€URLï¼‰ã‚’æ•´å½¢ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚`,
	Args:  cobra.NoArgs,

	RunE: func(cmd *cobra.Command, args []string) error {

		// Flags.TimeoutSec ã¯ cmd/root.go ã§å®šç¾©ã•ã‚Œã¦ã„ã¾ã™
		// å…¨ä½“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®2å€ (extractCmdã¨çµ±ä¸€)
		overallTimeout := time.Duration(Flags.TimeoutSec) * overallFeedTimeoutFactor * time.Second
		if Flags.TimeoutSec == 0 {
			overallTimeout = DefaultOverallTimeout
		}

		log.Printf("å‡¦ç†å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ‰URL: %s (å…¨ä½“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: %s)", feedURL, overallTimeout)

		// 1. ä¾å­˜æ€§ã®åˆæœŸåŒ–
		fetcher := GetGlobalFetcher()
		if fetcher == nil {
			// ğŸ’¡ ä¿®æ­£ç‚¹4: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰å†…éƒ¨å®Ÿè£…ã®è©³ç´°ã¸ã®è¨€åŠã‚’é¿ã‘ã‚‹
			return fmt.Errorf("HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
		}

		client, ok := fetcher.(*httpkit.Client)
		if !ok {
			return fmt.Errorf("äºˆæœŸã—ãªã„HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å®Ÿè£…ã§ã™: %Tã€‚feed.NewParserã¯*httpkit.Clientã‚’æœŸå¾…ã—ã¾ã™ã€‚", fetcher)
		}

		// NewParser ã‚’åˆ©ç”¨
		parser := feed.NewParser(client)

		// 3. ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè¡Œ
		parsedFeed, err := runParsePipeline(feedURL, parser, overallTimeout)
		if err != nil {
			return fmt.Errorf("ãƒ•ã‚£ãƒ¼ãƒ‰è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: %w", err)
		}

		// 4. çµæœã®å‡ºåŠ›
		fmt.Printf("--- ãƒ•ã‚£ãƒ¼ãƒ‰è§£æçµæœ ---\n")
		fmt.Printf("ãƒ•ã‚£ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«: %s\n", parsedFeed.Title)
		if parsedFeed.Link != "" {
			fmt.Printf("ãƒªãƒ³ã‚¯: %s\n", parsedFeed.Link)
		}
		fmt.Printf("åˆè¨ˆè¨˜äº‹æ•°: %d\n", len(parsedFeed.Items))
		fmt.Println("-----------------------")

		for i, item := range parsedFeed.Items {
			fmt.Printf("[%d] %s\n", i+1, item.Title)
			fmt.Printf("    URL: %s\n", item.Link)
			if item.PublishedParsed != nil {
				fmt.Printf("    å…¬é–‹æ—¥: %s\n", item.PublishedParsed.Local().Format("2006-01-02 15:04:05"))
			}
		}
		// æœ€å¾Œã«æ”¹è¡Œã‚’åŠ ãˆã¦å‡ºåŠ›å®Œäº†ã¨ã™ã‚‹
		fmt.Println()

		return nil
	},
}

func init() {
	// ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰å›ºæœ‰ã®ãƒ•ãƒ©ã‚°å®šç¾©
	parseCmd.Flags().StringVarP(&feedURL, "url", "u", "", "è§£æå¯¾è±¡ã®ãƒ•ã‚£ãƒ¼ãƒ‰ (RSS/Atom) URL")

	// URLãƒ•ãƒ©ã‚°ã‚’å¿…é ˆã«ã™ã‚‹
	parseCmd.MarkFlagRequired("url")
}
