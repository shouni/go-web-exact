package main

import (
	// ğŸ’¡ ä¿®æ­£ 1: æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€æœ€åˆã«é…ç½®
	"bufio"
	"context"
	"fmt"
	"log"
	"net/url"
	"os"
	"time"

	"github.com/shouni/go-http-kit/pkg/httpkit"
	"github.com/shouni/go-web-exact/v2/pkg/extract"
)

// runExtractionPipeline ã¯ã€Webã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡ºã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã§ã™ã€‚
// ğŸ’¡ ä¿®æ­£ 2: overallTimeout ã‚’å¼•æ•°ã¨ã—ã¦å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´
func runExtractionPipeline(rawURL string, extractor *extract.Extractor, overallTimeout time.Duration) (text string, hasBody bool, err error) {
	// 1. å…¨ä½“å‡¦ç†ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š
	ctx, cancel := context.WithTimeout(context.Background(), overallTimeout)
	defer cancel()

	// 2. æŠ½å‡ºã®å®Ÿè¡Œ
	text, hasBody, err = extractor.FetchAndExtractText(rawURL, ctx)
	if err != nil {
		return "", false, fmt.Errorf("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: %w", err)
	}

	return text, hasBody, nil
}

func main() {
	const overallTimeout = 60 * time.Second
	const clientTimeout = 30 * time.Second

	// 1. æ¨™æº–å…¥åŠ›ã‹ã‚‰URLã‚’èª­ã¿å–ã‚‹ (I/Oã®è²¬å‹™)
	scanner := bufio.NewScanner(os.Stdin)
	fmt.Print("å‡¦ç†ã™ã‚‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")

	if !scanner.Scan() {
		if err := scanner.Err(); err != nil {
			log.Fatalf("æ¨™æº–å…¥åŠ›ã®èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: %v", err)
		}
		log.Fatalf("URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
	}
	rawURL := scanner.Text()

	// 2. URLã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã‚¹ã‚­ãƒ¼ãƒ è£œå®Œ
	if rawURL == "" {
		log.Fatalf("ç„¡åŠ¹ãªURLãŒå…¥åŠ›ã•ã‚Œã¾ã—ãŸã€‚")
	}

	parsedURL, err := url.Parse(rawURL)
	if err != nil {
		log.Fatalf("URLã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: %v", err)
	}

	// ã‚¹ã‚­ãƒ¼ãƒ ãŒãªã„å ´åˆã€http:// ã‚’è£œå®Œã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ 
	// ğŸ’¡ ä¿®æ­£ 3: ". " ã®æ¡ä»¶ã‚’å‰Šé™¤ã—ã€ã‚¹ã‚­ãƒ¼ãƒ ãŒç©ºã®å ´åˆã®ã¿è£œå®Œã™ã‚‹
	if parsedURL.Scheme == "" {
		rawURL = "http://" + rawURL
		parsedURL, err = url.Parse(rawURL)
		if err != nil {
			log.Fatalf("URLã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ (ã‚¹ã‚­ãƒ¼ãƒ è£œå®Œå¾Œ): %v", err)
		}
	}

	if parsedURL.Scheme != "http" && parsedURL.Scheme != "https" {
		log.Fatalf("ç„¡åŠ¹ãªURLã‚¹ã‚­ãƒ¼ãƒ ã§ã™ã€‚httpã¾ãŸã¯httpsã‚’æŒ‡å®šã—ã¦ãã ã•ã„: %s", rawURL)
	}
	fmt.Printf("å…¥åŠ›ã•ã‚ŒãŸURL: %s\n", rawURL)

	// 3. ä¾å­˜æ€§ã®åˆæœŸåŒ– (DIã‚³ãƒ³ãƒ†ãƒŠã®å½¹å‰²)
	// clientTimeout ã‚’ä½¿ç”¨ã—ã¦ fetcher ã‚’åˆæœŸåŒ–
	fetcher := httpkit.New(clientTimeout, httpkit.WithMaxRetries(2))
	extractor, err := extract.NewExtractor(fetcher)
	if err != nil {
		log.Fatalf("Extractorã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: %v", err)
	}

	// 4. ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè¡Œ (ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’å‘¼ã³å‡ºã—)
	// ğŸ’¡ ä¿®æ­£ 2: overallTimeout ã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã™
	text, hasBody, err := runExtractionPipeline(rawURL, extractor, overallTimeout)

	if err != nil {
		log.Fatalf("å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: %v", err)
	}

	// 5. çµæœã®å‡ºåŠ›
	if !hasBody {
		fmt.Printf("æœ¬æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã—ã¾ã—ãŸ:\n%s\n", text)
	} else {
		fmt.Println("--- æŠ½å‡ºã•ã‚ŒãŸæœ¬æ–‡ ---")
		fmt.Println(text)
		fmt.Println("-----------------------")
	}
}
